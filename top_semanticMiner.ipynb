{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "top_semanticMiner.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88i2trkYWQCd"
      },
      "source": [
        "# A complete profile for sentiment analysis model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Bm1hVHTWQCf"
      },
      "source": [
        "## Models Ideas:\n",
        "1. Use of custom models for different categories (tech, food, books,...) to be automatically (using context classfication) or manually selected(by the client). *(different datasets applied)*\n",
        "2. Run multiple models per dataset and derive weighted average results.\n",
        "3. Developing a layered classification **use *fast/slow* classification** (divide the dataset using confidence index to strong and weak groups; the weak group will be analysed further using Roberta model).\n",
        "4. Aspect based analysis **(attach sentiment to specific aspects rather than sentence/opinion)** and word cloud **(for word frequencies)** to show insights of the reviews. (Amazon comprehend model)\n",
        "5. Use of lemmatization, opinion unit extractor, subjectivity index and multiclass classification(love, sad, angry,...) for better accuracy and data enrichment.\n",
        "6. Test of a sent-ngrams lexion sentiment analysis **(SO-CAL)**.\n",
        "7. Use of client dataset to fine-tune the model. (Ideation phase)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sodUERMXWQCg"
      },
      "source": [
        "## Datasets used:\n",
        "1. Twitter airline \n",
        "2. IMDB \n",
        "3. Yelp (preprocessing phase)\n",
        "4. Amazon \n",
        "5. 140sentiment twitter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnyUKCIYWQCh"
      },
      "source": [
        "## Implementation:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUP9j8SCWQCh"
      },
      "source": [
        "### Imports:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkIt7_lKp0h-"
      },
      "source": [
        "# Install the transformers library\n",
        "!pip install transformers\n",
        "!pip install vaderSentiment\n",
        "!pip install flair"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "YRHTAIVJWQCh"
      },
      "source": [
        "from textblob import TextBlob as tb\n",
        "from textblob.sentiments import NaiveBayesAnalyzer\n",
        "from textblob import Blobber\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from flair.data import Sentence\n",
        "from flair.models import TextClassifier\n",
        "from sklearn import metrics\n",
        "import pandas as pd\n",
        "from nltk import tokenize\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer\n",
        "# only for colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kE59-3sHWQCi"
      },
      "source": [
        "### libraries implementation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "NGERbWuDWQCi"
      },
      "source": [
        "def metric(true,predict):\n",
        "    analytics=[]\n",
        "    #metrics.classification_report(true,predict)\n",
        "    analytics.append(round(metrics.accuracy_score(true,predict),2))\n",
        "    analytics.append(round(metrics.precision_score(true,predict,average='weighted'),2))\n",
        "    analytics.append(round(metrics.recall_score(true,predict,average='weighted'),2))\n",
        "    analytics.append(round(metrics.f1_score(true,predict,average='weighted'),2))\n",
        "    return analytics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXdp_EIMWQCj"
      },
      "source": [
        "class modelDataset:\n",
        "    def __init__(self, tokenized_texts):\n",
        "        self.tokenized_texts = tokenized_texts\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokenized_texts[\"input_ids\"])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {k: v[idx] for k, v in self.tokenized_texts.items()}"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "sT28MtWhWQCj"
      },
      "source": [
        "def textblobPattern(text):\n",
        "    sentiment=[]\n",
        "    for sentence in text:\n",
        "        sent=tb(sentence).polarity\n",
        "        if sent>0:\n",
        "            sentiment.append(1)\n",
        "        elif sent<0:\n",
        "            sentiment.append(-1)\n",
        "        else:\n",
        "            sentiment.append(0)\n",
        "    return sentiment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "NxbBp013WQCj"
      },
      "source": [
        "def textblobNB(text):\n",
        "    sentiment=[]\n",
        "    tbnb = Blobber(analyzer=NaiveBayesAnalyzer())\n",
        "    for sentence in text:\n",
        "        ts=tbnb(sentence).sentiment\n",
        "    return sentiment    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "hWrVYGS0WQCk"
      },
      "source": [
        "def vader(text):\n",
        "    sentiment=[]\n",
        "    analyzer = SentimentIntensityAnalyzer()\n",
        "    for sentence in text:\n",
        "        vs=analyzer.polarity_scores(sentence)['compound']\n",
        "        if (vs > 0.5):\n",
        "            sentiment.append(1)\n",
        "        elif (vs < -0.5):\n",
        "            sentiment.append(-1)\n",
        "        else:\n",
        "            sentiment.append(0)\n",
        "    return sentiment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "nZ890oBzWQCk"
      },
      "source": [
        "def flair(text):\n",
        "    classifier = TextClassifier.load('sentiment-fast')\n",
        "    sentences = [Sentence(t) for t in text]\n",
        "    sentiment=[]\n",
        "    for phrase in sentences:\n",
        "        classifier.predict(phrase,mini_batch_size=32)\n",
        "        sentiment.append(1 if phrase.labels[0].value == 'POSITIVE' else -1)\n",
        "    return sentiment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "auHyGPYmWQCk"
      },
      "source": [
        "def RoBerta_large(text):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"siebert/sentiment-roberta-large-english\")\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\"siebert/sentiment-roberta-large-english\")\n",
        "    trainer = Trainer(model=model)\n",
        "    tokenized_texts = tokenizer(text,truncation=True,padding=True)\n",
        "    pred_dataset = modelDataset(tokenized_texts)\n",
        "    predictions = trainer.predict(pred_dataset)\n",
        "    preds = predictions.predictions.argmax(-1)\n",
        "    return [-1 if x==0 else 1 for x in preds]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "nw23RQKXWQCl"
      },
      "source": [
        "def roBerta_multitwitter(df):\n",
        "    # under processing\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrLV0saxWQCl"
      },
      "source": [
        "def bert_base(text):\n",
        "    text=['lol i am happy']\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
        "    trainer = Trainer(model=model)\n",
        "    for sentence in text:\n",
        "      inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "      output= model(**inputs)\n",
        "    # to continue"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8YPb8oaWQCn"
      },
      "source": [
        "### Model testing: (for each dataset)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kjInZ-uWujA"
      },
      "source": [
        "def UsAirTw():\n",
        "  file_name = \"/content/drive/MyDrive/usAir_tweets.csv\"\n",
        "  text_column = \"text\"\n",
        "  df = pd.read_csv(file_name)\n",
        "  true=df[\"sentiment\"]\n",
        "  pred_texts = df[text_column].dropna().astype('str').tolist()\n",
        "  textblobM=metric(true,textblobPattern(pred_texts))\n",
        "  vaderM=metric(true,vader(pred_texts))\n",
        "  flairM=metric(true,flair(pred_texts))\n",
        "  robertaM=metric(true,RoBerta_large(pred_texts))\n",
        "  print(f\"The textblob metrics:\\n accuracy={textblobM[0]},precision={textblobM[1]},recall={textblobM[2]},f1 score ={textblobM[3]}\")\n",
        "  print(f\"The Vader metrics:\\n accuracy={vaderM[0]},precision={vaderM[1]},recall={vaderM[2]},f1 score ={vaderM[3]}\")\n",
        "  print(f\"The flair metrics:\\n accuracy={flairM[0]},precision={flairM[1]},recall={flairM[2]},f1 score ={flairM[3]}\")\n",
        "  print(f\"The Roberta large metrics:\\n accuracy={robertaM[0]},precision={robertaM[1]},recall={robertaM[2]},f1 score ={robertaM[3]}\")\n",
        "\n",
        "def Imdb():\n",
        "  file_name = \"/content/drive/MyDrive/imdb.csv\"\n",
        "  text_column = \"review\"\n",
        "  df = pd.read_csv(file_name)\n",
        "  true=df[\"sentiment\"]\n",
        "  pred_texts = df[text_column].dropna().astype('str').tolist()\n",
        "  textblobM=metric(true,textblobPattern(pred_texts))\n",
        "  vaderM=metric(true,vader(pred_texts))\n",
        "  flairM=metric(true,flair(pred_texts))\n",
        "  robertaM=metric(true,RoBerta_large(pred_texts))\n",
        "  print(f\"The textblob metrics:\\n accuracy={textblobM[0]},precision={textblobM[1]},recall={textblobM[2]},f1 score ={textblobM[3]}\")\n",
        "  print(f\"The Vader metrics:\\n accuracy={vaderM[0]},precision={vaderM[1]},recall={vaderM[2]},f1 score ={vaderM[3]}\")\n",
        "  print(f\"The flair metrics:\\n accuracy={flairM[0]},precision={flairM[1]},recall={flairM[2]},f1 score ={flairM[3]}\")\n",
        "  print(f\"The Roberta large metrics:\\n accuracy={robertaM[0]},precision={robertaM[1]},recall={robertaM[2]},f1 score ={robertaM[3]}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDsZsiloWQCn"
      },
      "source": [
        "### Main:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7ngJZhTcuM-"
      },
      "source": [
        "UsAirTw()\n",
        "Imdb()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMFt-OUiWQCo"
      },
      "source": [
        "## Results:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQQ4l1NvWQCo"
      },
      "source": [
        "## Topics covered:\n",
        "- Textblob - vader - flair libraries\n",
        "- Text operations: lemmatization - tokenization - vectorization - wordnet - tagging - n-gram \n",
        "- Machine learning concepts: vector space model, k-means clustering,[ Naive Bayes, k-NN, SVM] classifiers, decision tree - random forest - transformers (word2vec and wordtree of stanford).\n",
        "- Technologies: Jupyter notebook - Google colab\n",
        "- Dataset handeling: dataset preprocessing\n",
        "- Sentiment analysis approaches\n",
        "- Handeling multiple  Deeplearning models: roBERTa - BERT - [GloVe - Fasttext - torchtext]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmNKHYLpWQCp"
      },
      "source": [
        "## References:\n",
        "- https://neptune.ai/blog/sentiment-analysis-python-textblob-vs-vader-vs-flair\n",
        "- https://towardsdatascience.com/customer-churn-accuracy-a-4-6-increase-with-feature-engineering-29bcb1b1ee8f (REVIEW)\n",
        "- https://www.analyticsvidhya.com/blog/2021/01/sentiment-analysis-vader-or-textblob/\n",
        "- https://pythonprogramming.net/sentiment-analysis-python-textblob-vader/\n",
        "- https://towardsdatascience.com/sentimental-analysis-using-vader-a3415fef7664\n",
        "- https://medium.com/geekculture/what-nlp-library-you-should-use-for-your-sentimental-analysis-project-bef6b357a6db\n",
        "- https://towardsdatascience.com/sentiment-analysis-comparing-3-common-approaches-naive-bayes-lstm-and-vader-ab561f834f89\n",
        "****\n",
        "* N-grams rule based model\n",
        "- https://www.sciencedirect.com/science/article/pii/S095741741830143X\n",
        "- https://github.com/sfu-discourse-lab/SO-CAL(to be reviewed)\n",
        "- https://towardsdatascience.com/text-analysis-basics-in-python-443282942ec5\n",
        "****\n",
        "- https://towardsdatascience.com/text-classification-with-state-of-the-art-nlp-library-flair-b541d7add21f"
      ]
    }
  ]
}