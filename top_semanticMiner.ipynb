{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88i2trkYWQCd"
   },
   "source": [
    "# A complete profile for sentiment analysis model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Bm1hVHTWQCf"
   },
   "source": [
    "## Models Ideas:\n",
    "1. Use of custom models for different categories (tech, food, books,...) to be automatically (using context classfication) or manually selected(by the client). *(different datasets applied)*\n",
    "2. Run multiple models per dataset and derive weighted average results.\n",
    "3. Developing a layered classification **use *fast/slow* classification** (divide the dataset using confidence index to strong and weak groups; the weak group will be analysed further using Roberta model).\n",
    "4. Aspect based analysis **(attach sentiment to specific aspects rather than sentence/opinion)** and word cloud **(for word frequencies)** to show insights of the reviews. (Amazon comprehend model)\n",
    "5. Use of lemmatization, opinion unit extractor, subjectivity index and multiclass classification(love, sad, angry,...) for better accuracy and data enrichment.\n",
    "6. Test of a sent-ngrams lexion sentiment analysis **(SO-CAL)**.\n",
    "7. Use of client dataset to fine-tune the model. (Ideation phase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sodUERMXWQCg"
   },
   "source": [
    "## Datasets used:\n",
    "1. Twitter airline \n",
    "2. IMDB \n",
    "3. Yelp (preprocessing phase)\n",
    "4. Amazon \n",
    "5. 140sentiment twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VnyUKCIYWQCh"
   },
   "source": [
    "## Implementation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AUP9j8SCWQCh"
   },
   "source": [
    "### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zkIt7_lKp0h-",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Install the transformers library\n",
    "!pip install transformers\n",
    "!pip install vaderSentiment\n",
    "!pip install flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YRHTAIVJWQCh",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob as tb\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "from textblob import Blobber\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from flair.data import Sentence\n",
    "from flair.models import TextClassifier\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer\n",
    "# only for colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kE59-3sHWQCi"
   },
   "source": [
    "### libraries implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NGERbWuDWQCi",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def metric(true,predict):\n",
    "    analytics=[]\n",
    "    #metrics.classification_report(true,predict)\n",
    "    analytics.append(round(metrics.accuracy_score(true,predict),2))\n",
    "    analytics.append(round(metrics.precision_score(true,predict,average='weighted'),2))\n",
    "    analytics.append(round(metrics.recall_score(true,predict,average='weighted'),2))\n",
    "    analytics.append(round(metrics.f1_score(true,predict,average='weighted'),2))\n",
    "    return analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oXdp_EIMWQCj",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# class modelDataset:\n",
    "#     def __init__(self, tokenized_texts):\n",
    "#         self.tokenized_texts = tokenized_texts\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.tokenized_texts[\"input_ids\"])\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         return {k: v[idx] for k, v in self.tokenized_texts.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sT28MtWhWQCj",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def textblobPattern(text):\n",
    "    sentiment = []\n",
    "    start=time.time()\n",
    "    for sentence in text:\n",
    "        sent = tb(sentence).polarity\n",
    "        if sent > 0:\n",
    "            sentiment.append(1)\n",
    "        elif sent < 0:\n",
    "            sentiment.append(-1)\n",
    "        else:\n",
    "            sentiment.append(0)\n",
    "    end=time.time()\n",
    "    tm=(end-start)/len(text)\n",
    "    return tm,sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def textblobNB(text):\n",
    "    sentimentV, sentimentT = [], []\n",
    "    tbnb = Blobber(analyzer=NaiveBayesAnalyzer())\n",
    "    for sentence in text:\n",
    "        ts = tbnb(sentence).sentiment\n",
    "    return sentimentV, sentimentT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def vader(text):\n",
    "    sentiment = []\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    start=time.time()\n",
    "    for sentence in text:\n",
    "        vs = analyzer.polarity_scores(sentence)['compound']\n",
    "        if (vs > 0.5):\n",
    "            sentiment.append(1)\n",
    "        elif (vs < -0.5):\n",
    "            sentiment.append(-1)\n",
    "        else:\n",
    "            sentiment.append(0)\n",
    "    end = time.time()\n",
    "    tm = (end - start) / len(text)\n",
    "    return tm, sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def flair(text):\n",
    "    classifier = TextClassifier.load('sentiment-fast')\n",
    "    # sentences = [Sentence(t) for t in text]\n",
    "    sentiment = []\n",
    "    start=time.time()\n",
    "    for phrase in text:\n",
    "        sentence = Sentence(phrase)\n",
    "        classifier.predict(sentence, mini_batch_size=32)\n",
    "        sentiment.append(1 if sentence.labels[0].value == 'POSITIVE' else -1)\n",
    "    end = time.time()\n",
    "    tm = (end - start) / len(text)\n",
    "    return tm, sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def RoBerta_large(text):\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(\"siebert/sentiment-roberta-large-english\")\n",
    "    # model = AutoModelForSequenceClassification.from_pretrained(\"siebert/sentiment-roberta-large-english\")\n",
    "    # trainer = Trainer(model=model)\n",
    "    # tokenized_texts = tokenizer(text,truncation=True,padding=True)\n",
    "    # pred_dataset = modelDataset(tokenized_texts)\n",
    "    # predictions = trainer.predict(pred_dataset)\n",
    "    # preds = predictions.predictions.argmax(-1)\n",
    "    # return [-1 if x==0 else 1 for x in preds]\n",
    "    sentiment = []\n",
    "    sentiment_analysis = pipeline(\"sentiment-analysis\", model=\"siebert/sentiment-roberta-large-english\")\n",
    "    start=time.time()\n",
    "    for x in text:\n",
    "        sentiment.append(1 if sentiment_analysis(x, truncation=True, padding=True)[0]['label'] == 'POSITIVE' else -1)\n",
    "    end = time.time()\n",
    "    tm = (end - start) / len(text)\n",
    "    return tm, sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def bert_base(text):\n",
    "    sentiment = []\n",
    "    sentiment_analysis = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "    start=time.time()\n",
    "    for x in text:\n",
    "        sent=  (sentiment_analysis(x, truncation=True, padding=True)[0]['label'])\n",
    "        if sent== \"1 star\":\n",
    "                sentiment.append(-1)\n",
    "        elif sent== \"2 stars\":\n",
    "                sentiment.append(-1)\n",
    "        elif sent == \"3 stars\":\n",
    "                sentiment.append(0)\n",
    "        elif sent ==\"4 stars\":\n",
    "                sentiment.append(1)\n",
    "        elif sent ==\"5 stars\":\n",
    "                sentiment.append(1)\n",
    "    end = time.time()\n",
    "    tm = (end - start) / len(text)\n",
    "    return tm, sentiment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model testing: (for each dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def UsAirTw():\n",
    "    file_name = \"Datasets/usAir_tweets.csv\"\n",
    "    text_column = \"text\"\n",
    "    df = pd.read_csv(file_name)\n",
    "    df=df[:10000]\n",
    "    true = df[\"sentiment\"]\n",
    "    pred_texts = df[text_column].dropna().astype('str').tolist()\n",
    "    print(\"Metrics for usairline_tweets Dataset:\")\n",
    "    time, pred = textblobPattern(pred_texts)\n",
    "    textblobM = metric(true, pred)\n",
    "    print(f\"The textblob metrics:\\n accuracy={textblobM[0]},precision={textblobM[1]},recall={textblobM[2]},f1 score ={textblobM[3]},time per sentiment= {time}\")\n",
    "    time, pred = vader(pred_texts)\n",
    "    vaderM = metric(true, pred)\n",
    "    print(f\"The Vader metrics:\\n accuracy={vaderM[0]},precision={vaderM[1]},recall={vaderM[2]},f1 score ={vaderM[3]},time per sentiment= {time}\")\n",
    "    time, pred = flair(pred_texts)\n",
    "    flairM = metric(true, pred)\n",
    "    print(f\"The flair metrics:\\n accuracy={flairM[0]},precision={flairM[1]},recall={flairM[2]},f1 score ={flairM[3]},time per sentiment= {time}\")\n",
    "    time, pred = RoBerta_large(pred_texts)\n",
    "    robertaM = metric(true, pred)\n",
    "    print(f\"The Roberta large metrics:\\n accuracy={robertaM[0]},precision={robertaM[1]},recall={robertaM[2]},f1 score ={robertaM[3]},time per sentiment= {time}\")\n",
    "    time, pred = bert_base(pred_texts)\n",
    "    Bert = metric(true, pred)\n",
    "    print(f\"The bert multilang metrics:\\n accuracy={Bert[0]},precision={Bert[1]},recall={Bert[2]},f1 score ={Bert[3]},time per sentiment= {time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def yelp():\n",
    "    file_name = \"Datasets/yelp.csv\"\n",
    "    text_column = \"text\"\n",
    "    df = pd.read_csv(file_name)\n",
    "    true = df[\"sent\"]\n",
    "    pred_texts = df[text_column].dropna().astype('str').tolist()\n",
    "    print(\"Metrics for yelp sentiment:\")\n",
    "    time, pred = textblobPattern(pred_texts)\n",
    "    textblobM = metric(true, pred)\n",
    "    print(f\"The textblob metrics:\\n accuracy={textblobM[0]},precision={textblobM[1]},recall={textblobM[2]},f1 score ={textblobM[3]},time per sentiment= {time}\")\n",
    "    time, pred = vader(pred_texts)\n",
    "    vaderM = metric(true, pred)\n",
    "    print(f\"The Vader metrics:\\n accuracy={vaderM[0]},precision={vaderM[1]},recall={vaderM[2]},f1 score ={vaderM[3]},time per sentiment= {time}\")\n",
    "    time, pred = flair(pred_texts)\n",
    "    flairM = metric(true, pred)\n",
    "    print(f\"The flair metrics:\\n accuracy={flairM[0]},precision={flairM[1]},recall={flairM[2]},f1 score ={flairM[3]},time per sentiment= {time}\")\n",
    "    time, pred = RoBerta_large(pred_texts)\n",
    "    robertaM = metric(true, pred)\n",
    "    print(f\"The Roberta large metrics:\\n accuracy={robertaM[0]},precision={robertaM[1]},recall={robertaM[2]},f1 score ={robertaM[3]},time per sentiment= {time}\")\n",
    "    time, pred = bert_base(pred_texts)\n",
    "    Bert = metric(true, pred)\n",
    "    print(f\"The bert multilang metrics:\\n accuracy={Bert[0]},precision={Bert[1]},recall={Bert[2]},f1 score ={Bert[3]},time per sentiment= {time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def Imdb():\n",
    "    file_name = \"Datasets/Imdb.csv\"\n",
    "    text_column = \"review\"\n",
    "    df = pd.read_csv(file_name)\n",
    "    df=df[:10000]\n",
    "    true = df[\"sentiment\"]\n",
    "    pred_texts = df[text_column].dropna().astype('str').tolist()\n",
    "    print(\"Metrics for IMDB dataset:\")\n",
    "    time, pred = textblobPattern(pred_texts)\n",
    "    textblobM = metric(true, pred)\n",
    "    print(f\"The textblob metrics:\\n accuracy={textblobM[0]},precision={textblobM[1]},recall={textblobM[2]},f1 score ={textblobM[3]},time per sentimen= t{time}\")\n",
    "    time, pred = vader(pred_texts)\n",
    "    vaderM = metric(true, pred)\n",
    "    print(f\"The Vader metrics:\\n accuracy={vaderM[0]},precision={vaderM[1]},recall={vaderM[2]},f1 score ={vaderM[3]},time per sentiment= {time}\")\n",
    "    time, pred = flair(pred_texts)\n",
    "    flairM = metric(true, pred)\n",
    "    print(f\"The flair metrics:\\n accuracy={flairM[0]},precision={flairM[1]},recall={flairM[2]},f1 score ={flairM[3]},time per sentiment= {time}\")\n",
    "    time, pred = RoBerta_large(pred_texts)\n",
    "    robertaM = metric(true, pred)\n",
    "    print(f\"The Roberta large metrics:\\n accuracy={robertaM[0]},precision={robertaM[1]},recall={robertaM[2]},f1 score ={robertaM[3]},time per sentiment= {time}\")\n",
    "    time, pred = bert_base(pred_texts)\n",
    "    Bert = metric(true, pred)\n",
    "    print(f\"The bert multilang metrics:\\n accuracy={Bert[0]},precision={Bert[1]},recall={Bert[2]},f1 score ={Bert[3]},time per sentiment= {time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def t140sentiment():\n",
    "    file_name = \"/content/drive/MyDrive/140sentiment.csv\"\n",
    "    text_column = \"text\"\n",
    "    df = pd.read_csv(file_name)\n",
    "    df=df[:10000]\n",
    "    true = df[\"sent\"]\n",
    "    pred_texts = df[text_column].dropna().astype('str').tolist()\n",
    "    print(\"Metrics for sentiment140 dataset:\")\n",
    "    time, pred = textblobPattern(pred_texts)\n",
    "    textblobM = metric(true, pred)\n",
    "    print(f\"The textblob metrics:\\n accuracy={textblobM[0]},precision={textblobM[1]},recall={textblobM[2]},f1 score ={textblobM[3]},time per sentiment= {time}\")\n",
    "    time, pred = vader(pred_texts)\n",
    "    vaderM = metric(true, pred)\n",
    "    print(f\"The Vader metrics:\\n accuracy={vaderM[0]},precision={vaderM[1]},recall={vaderM[2]},f1 score ={vaderM[3]},time per sentiment= {time}\")\n",
    "    time, pred = flair(pred_texts)\n",
    "    flairM = metric(true, pred)\n",
    "    print(f\"The flair metrics:\\n accuracy={flairM[0]},precision={flairM[1]},recall={flairM[2]},f1 score ={flairM[3]},time per sentiment= {time}\")\n",
    "    time, pred = RoBerta_large(pred_texts)\n",
    "    robertaM = metric(true, pred)\n",
    "    print(f\"The Roberta large metrics:\\n accuracy={robertaM[0]},precision={robertaM[1]},recall={robertaM[2]},f1 score ={robertaM[3]},time per sentiment= {time}\")\n",
    "    time, pred = bert_base(pred_texts)\n",
    "    Bert = metric(true, pred)\n",
    "    print( f\"The bert multilang metrics:\\n accuracy={Bert[0]},precision={Bert[1]},recall={Bert[2]},f1 score ={Bert[3]},time per sentiment= {time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def amazon():\n",
    "    file_name = \"Datasets/amazonFoods.csv\"\n",
    "    text_column = \"Text\"\n",
    "    df = pd.read_csv(file_name)\n",
    "    df=df[:10000]\n",
    "    true = df[\"Score\"]\n",
    "    pred_texts = df[text_column].dropna().astype('str').tolist()\n",
    "    print(\"Metrics for Amazon food dataset:\")\n",
    "    time,pred=textblobPattern(pred_texts)\n",
    "    textblobM = metric(true, pred)\n",
    "    print(f\"The textblob metrics:\\n accuracy={textblobM[0]},precision={textblobM[1]},recall={textblobM[2]},f1 score ={textblobM[3]},time per sentiment= {time}\")\n",
    "    time,pred=vader(pred_texts)\n",
    "    vaderM = metric(true, pred)\n",
    "    print(f\"The Vader metrics:\\n accuracy={vaderM[0]},precision={vaderM[1]},recall={vaderM[2]},f1 score ={vaderM[3]},time per sentiment= {time}\")\n",
    "    time,pred=flair(pred_texts)\n",
    "    flairM = metric(true, pred)\n",
    "    print(f\"The flair metrics:\\n accuracy={flairM[0]},precision={flairM[1]},recall={flairM[2]},f1 score ={flairM[3]},time per sentiment= {time}\")\n",
    "    time,pred=RoBerta_large(pred_texts)\n",
    "    robertaM = metric(true, pred)\n",
    "    print(f\"The Roberta large metrics:\\n accuracy={robertaM[0]},precision={robertaM[1]},recall={robertaM[2]},f1 score ={robertaM[3]},time per sentiment= {time}\")\n",
    "    time,pred=bert_base(pred_texts)\n",
    "    Bert = metric(true, pred)\n",
    "    print(f\"The bert multilang metrics:\\n accuracy={Bert[0]},precision={Bert[1]},recall={Bert[2]},f1 score ={Bert[3]},time per sentiment= {time}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G7ngJZhTcuM-",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "UsAirTw()\n",
    "Imdb()\n",
    "t140sentiment()\n",
    "amazon()\n",
    "yelp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CMFt-OUiWQCo"
   },
   "source": [
    "## Results:\n",
    "### for UsAirlineTweets:\n",
    "- The textblob metrics:\n",
    " - accuracy=0.47, precision=0.68, recall=0.47, f1 score =0.48,time per sentiment = 0.0001\n",
    "- The Vader metrics:\n",
    " - accuracy=0.41, precision=0.73, recall=0.41, f1 score =0.4,time per sentiment = 0.0001\n",
    "- The flair metrics:\n",
    " - accuracy=0.67, precision=0.58, recall=0.67, f1 score =0.61 ,time per sentiment = 0.001\n",
    "- The Roberta large metrics:\n",
    " - accuracy=0.75, precision=0.62, recall=0.75, f1 score =0.67,time per sentiment = 0.0173\n",
    "- The bert multilang metrics:\n",
    " - accuracy=0.66, precision=0.64, recall=0.66, f1 score =0.64,time per sentiment = 0.071\n",
    " \n",
    "### for imdb:\n",
    "- The textblob metrics:\n",
    " - accuracy=0.69,precision=0.76,recall=0.69,f1 score =0.67,time per sentimenT= 0.0015\n",
    "- The Vader metrics:\n",
    " - accuracy=0.64,precision=0.75,recall=0.64,f1 score =0.67,time per sentiment= 0.0064\n",
    "- The flair metrics:\n",
    " - accuracy=0.9,precision=0.9,recall=0.9,f1 score =0.9,time per sentiment= 0.039\n",
    "- The Roberta large metrics:\n",
    " - accuracy=0.96,precision=0.96,recall=0.96,f1 score =0.96,time per sentiment= 1.29\n",
    "\n",
    "### for Amazon:\n",
    "- The textblob metrics:\n",
    " - accuracy=0.79,precision=0.73,recall=0.79,f1 score =0.75,time per sentiment= 0.001\n",
    "- The Vader metrics:\n",
    " - accuracy=0.75,precision=0.79,recall=0.75,f1 score =0.75,time per sentiment= 0.001\n",
    "- The flair metrics:\n",
    " - accuracy=0.84,precision=0.82,recall=0.84,f1 score =0.82,time per sentiment= 0.013\n",
    "- The Roberta large metrics:\n",
    " - accuracy=0.89,precision=0.84,recall=0.89,f1 score =0.86,time per sentiment= 0.97\n",
    "- The bert multilang metrics:\n",
    " - accuracy=0.85,precision=0.88,recall=0.85,f1 score =0.86,time per sentiment= 0.29\n",
    "\n",
    "### for 140sentiment:\n",
    "- The textblob metrics:\n",
    " - accuracy=0.44,precision=0.7,recall=0.44,f1 score =0.52,time per sentiment= 0.00026\n",
    "- The Vader metrics:\n",
    " - accuracy=0.3,precision=0.79,recall=0.3,f1 score =0.41,time per sentiment= 0.0001\n",
    "- The flair metrics:\n",
    " - accuracy=0.7,precision=0.7,recall=0.7,f1 score =0.7,time per sentiment= 0.0031\n",
    " \n",
    "### for yelp:\n",
    "- The textblob metrics:\n",
    " - accuracy=0.68,precision=0.78,recall=0.68,f1 score =0.65,time per sentiment= 0.001\n",
    "- The Vader metrics:\n",
    " - accuracy=0.62,precision=0.83,recall=0.62,f1 score =0.63,time per sentiment= 0.002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQQ4l1NvWQCo"
   },
   "source": [
    "## Topics covered:\n",
    "- Textblob - vader - flair libraries\n",
    "- Text operations: lemmatization - tokenization - vectorization - wordnet - tagging - n-gram \n",
    "- Machine learning concepts: vector space model, k-means clustering,[ Naive Bayes, k-NN, SVM] classifiers, decision tree - random forest - transformers (word2vec and wordtree of stanford).\n",
    "- Technologies: Jupyter notebook - Google colab\n",
    "- Dataset handeling: dataset preprocessing\n",
    "- Sentiment analysis approaches\n",
    "- Handeling multiple  Deeplearning models: roBERTa - BERT - [GloVe - Fasttext - torchtext]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RmNKHYLpWQCp"
   },
   "source": [
    "## References:\n",
    "- https://neptune.ai/blog/sentiment-analysis-python-textblob-vs-vader-vs-flair\n",
    "- https://towardsdatascience.com/customer-churn-accuracy-a-4-6-increase-with-feature-engineering-29bcb1b1ee8f (REVIEW)\n",
    "- https://www.analyticsvidhya.com/blog/2021/01/sentiment-analysis-vader-or-textblob/\n",
    "- https://pythonprogramming.net/sentiment-analysis-python-textblob-vader/\n",
    "- https://towardsdatascience.com/sentimental-analysis-using-vader-a3415fef7664\n",
    "- https://medium.com/geekculture/what-nlp-library-you-should-use-for-your-sentimental-analysis-project-bef6b357a6db\n",
    "- https://towardsdatascience.com/sentiment-analysis-comparing-3-common-approaches-naive-bayes-lstm-and-vader-ab561f834f89\n",
    "****\n",
    "* N-grams rule based model\n",
    "- https://www.sciencedirect.com/science/article/pii/S095741741830143X\n",
    "- https://github.com/sfu-discourse-lab/SO-CAL(to be reviewed)\n",
    "- https://towardsdatascience.com/text-analysis-basics-in-python-443282942ec5\n",
    "****\n",
    "- https://towardsdatascience.com/text-classification-with-state-of-the-art-nlp-library-flair-b541d7add21f"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "top_semanticMiner.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
